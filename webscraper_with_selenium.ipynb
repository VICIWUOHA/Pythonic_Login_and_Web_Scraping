{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import io\n",
    "import requests as r\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from config import APP_BASE_URL,EMAIL,PASSWORD, CATEGORY_1, CATEGORY_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Driver\n",
    "driver = webdriver.Chrome(service = Service(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Running script\n",
      "Done Extracting Table_headers ..... Navigating to Inventory_Page to scrape data\n",
      "now fetching data from next page : 2\n",
      "now fetching data from next page : 3\n",
      "now fetching data from next page : 4\n",
      "now fetching data from next page : 5\n",
      "now fetching data from next page : 6\n",
      "now fetching data from next page : 7\n",
      "Seems there are no other pages: or  Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//li[@class=\"page-item next\"]/a\"}\n",
      "  (Session info: chrome=101.0.4951.67)\n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x0089B8F3+2406643]\n",
      "\tOrdinal0 [0x0082AF31+1945393]\n",
      "\tOrdinal0 [0x0071C748+837448]\n",
      "\tOrdinal0 [0x007492E0+1020640]\n",
      "\tOrdinal0 [0x0074957B+1021307]\n",
      "\tOrdinal0 [0x00776372+1205106]\n",
      "\tOrdinal0 [0x007642C4+1131204]\n",
      "\tOrdinal0 [0x00774682+1197698]\n",
      "\tOrdinal0 [0x00764096+1130646]\n",
      "\tOrdinal0 [0x0073E636+976438]\n",
      "\tOrdinal0 [0x0073F546+980294]\n",
      "\tGetHandleVerifier [0x00B09612+2498066]\n",
      "\tGetHandleVerifier [0x00AFC920+2445600]\n",
      "\tGetHandleVerifier [0x00934F2A+579370]\n",
      "\tGetHandleVerifier [0x00933D36+574774]\n",
      "\tOrdinal0 [0x00831C0B+1973259]\n",
      "\tOrdinal0 [0x00836688+1992328]\n",
      "\tOrdinal0 [0x00836775+1992565]\n",
      "\tOrdinal0 [0x0083F8D1+2029777]\n",
      "\tBaseThreadInitThunk [0x7766FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77B27A7E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77B27A4E+238]\n",
      " \n",
      "\n",
      " Moving to next Category: SD Finished Product.............\n",
      "now fetching data from next page : 2\n",
      "now fetching data from next page : 3\n",
      "now fetching data from next page : 4\n",
      "now fetching data from next page : 5\n",
      "now fetching data from next page : 6\n",
      "Seems there are no other pages: or  Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//li[@class=\"page-item next\"]/a\"}\n",
      "  (Session info: chrome=101.0.4951.67)\n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x0089B8F3+2406643]\n",
      "\tOrdinal0 [0x0082AF31+1945393]\n",
      "\tOrdinal0 [0x0071C748+837448]\n",
      "\tOrdinal0 [0x007492E0+1020640]\n",
      "\tOrdinal0 [0x0074957B+1021307]\n",
      "\tOrdinal0 [0x00776372+1205106]\n",
      "\tOrdinal0 [0x007642C4+1131204]\n",
      "\tOrdinal0 [0x00774682+1197698]\n",
      "\tOrdinal0 [0x00764096+1130646]\n",
      "\tOrdinal0 [0x0073E636+976438]\n",
      "\tOrdinal0 [0x0073F546+980294]\n",
      "\tGetHandleVerifier [0x00B09612+2498066]\n",
      "\tGetHandleVerifier [0x00AFC920+2445600]\n",
      "\tGetHandleVerifier [0x00934F2A+579370]\n",
      "\tGetHandleVerifier [0x00933D36+574774]\n",
      "\tOrdinal0 [0x00831C0B+1973259]\n",
      "\tOrdinal0 [0x00836688+1992328]\n",
      "\tOrdinal0 [0x00836775+1992565]\n",
      "\tOrdinal0 [0x0083F8D1+2029777]\n",
      "\tBaseThreadInitThunk [0x7766FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77B27A7E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77B27A4E+238]\n",
      " \n",
      "\n",
      " Moving to next Category: SD Finished Product.............\n",
      "Preview of Top 5 Rows: \n",
      " *---**---**---**---**---**---**---**---**---**---* \n",
      "\n",
      "                                  Name Unit Name Cost / Unit (NGN) Supplier\n",
      "1       Apple Cinnamon Shell Doughnut      Each           139.614        -\n",
      "2  Apple Cinnamon Shell Oreo Doughnut      Each           139.614        -\n",
      "3              Apple Fritter (PI2650)      Each             281.9        -\n",
      "4                   Banana Bread Cake      Each             119.9        -\n",
      "5      Banana Bread Muffin (KKPI2600)      Each           332.522        -\n",
      "Preview of Bottom 5 Rows: \n",
      " *---**---**---**---**---**---**---**---**---**---* \n",
      "\n",
      "                           Name Unit Name Cost / Unit (NGN) Supplier\n",
      "5                Waffle Royale        Oz            55.118        -\n",
      "6  White Chocolate  (FOOD0161)        Oz            249.28        -\n",
      "7  Wild Strawberry  (FOOD0169)        Oz            79.222        -\n",
      "8    Yoghurt Sprint (FOOD0172)        Oz           127.165        -\n",
      "9                   Zobo Crush        Oz            20.378        -\n",
      "Exporting 231 rows of data to Csv\n",
      "\n",
      "This took 124.01951456069946 seconds.\n"
     ]
    }
   ],
   "source": [
    "def login_and_get_table(EMAIL, PASSWORD, driver):\n",
    "    # driver.close()\n",
    "    driver.get(APP_BASE_URL+'/users/sign_in')\n",
    "    # find username/email field and send the username itself to the input field\n",
    "    driver.find_element(By.ID,\"user_email\").send_keys(EMAIL)\n",
    "    # find password input field and insert password as well\n",
    "    driver.find_element(By.ID,\"user_password\").send_keys(PASSWORD)\n",
    "    # click submit button which has tag \"mt-5\"\n",
    "    driver.find_element(By.CLASS_NAME,\"mt-5\").click()\n",
    "    # navigate to actual page to scrape data\n",
    "    driver.get(APP_BASE_URL+'/setup_inventory')\n",
    "    sleep(3)\n",
    "    driver.find_element(By.CLASS_NAME,\"ingredientsList\").click()\n",
    "    inventory_page = driver.page_source\n",
    "    raw_page_html = BeautifulSoup(inventory_page   ,'lxml')\n",
    "\n",
    "    table_headers = []\n",
    "    for table_header in raw_page_html.find_all('th'):\n",
    "        col_name = table_header.text\n",
    "        table_headers.append(col_name)   #Append all headers found to a list. \n",
    "    table_headers= table_headers[:-1]    #excluding last column header which had button links\n",
    "    return table_headers\n",
    "\n",
    "\n",
    "def extract_and_transform(table_headers):\n",
    "\n",
    "    Categs_to_Extract = [CATEGORY_1, CATEGORY_2]\n",
    "\n",
    "    extracted_inventory = pd.DataFrame()\n",
    "    for category in Categs_to_Extract:\n",
    "\n",
    "        driver.find_element(By.CLASS_NAME,\"filterByCategDropdown\").click()\n",
    "        sleep(2)\n",
    "        driver.find_element(By.PARTIAL_LINK_TEXT, category).click()\n",
    "\n",
    "        all_inventory_data = pd.DataFrame()\n",
    "        next_page_exists = True\n",
    "        page = 1\n",
    "        while (next_page_exists):\n",
    "            try:\n",
    "                page_info = driver.page_source\n",
    "                page_info_html =  BeautifulSoup(page_info,'lxml')\n",
    "                table_data = []\n",
    "                \n",
    "                for table_row in page_info_html.find_all('tr'):\n",
    "                    rows = {}\n",
    "                    for cell_data, table_header in zip(table_row.find_all('td'),table_headers):\n",
    "                        rows[table_header] = cell_data.text.replace('\\n','').strip()\n",
    "                    table_data.append(rows)\n",
    "                page_data_extracted = pd.DataFrame(table_data)\n",
    "                all_inventory_data = pd.concat([all_inventory_data,page_data_extracted])\n",
    "                sleep(5)\n",
    "                next_page = driver.find_element(By.XPATH,'//li[@class=\"page-item next\"]/a').click()\n",
    "                sleep(2)\n",
    "                page+=1\n",
    "                print('now fetching data from next page : {}'.format(page))\n",
    "            except Exception as E:\n",
    "                print('Seems there are no other pages: or ', E,'\\n\\n', 'Moving to next Category: {}.............'.format(Categs_to_Extract[1]))\n",
    "                extracted_inventory = pd.concat([extracted_inventory,all_inventory_data])\n",
    "                extracted_inventory.dropna(axis = 0, subset =['Name'],inplace=True)\n",
    "                next_page_exists = False\n",
    "                sleep(3)\n",
    "                break\n",
    "    return extracted_inventory\n",
    "\n",
    "def show_previews_and_export(extracted_inventory):\n",
    "    print('Preview of Top 5 Rows:','\\n','*---*'*10,'\\n\\n',extracted_inventory.head(5) )\n",
    "    sleep(3)\n",
    "    print('Preview of Bottom 5 Rows:','\\n','*---*'*10,'\\n\\n',extracted_inventory.tail(5) )\n",
    "    sleep(3)\n",
    "    print('Exporting {} rows of data to Csv'.format(len(extracted_inventory)))\n",
    "    extracted_inventory.to_csv('extracted_inventory_data.csv',index=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "  start_time = time.time()\n",
    "  print('Now Running script')\n",
    "  table_headers = login_and_get_table(EMAIL=EMAIL,PASSWORD= PASSWORD, driver=driver)\n",
    "  print('Done Extracting Table_headers ..... Navigating to Inventory_Page to scrape data')\n",
    "  extracted_inventory = extract_and_transform(table_headers)\n",
    "  show_previews_and_export(extracted_inventory)\n",
    "  print(\"\\nThis took %s seconds.\" % (time.time() - start_time))\n",
    "  return 'Done with Webscraping'       \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tmain()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2aafc8b317f9843068624b61f9604f1c5711d8aa460106b72f5abe10822f7ca8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
