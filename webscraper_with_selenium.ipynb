{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import io\n",
    "import requests as r\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from config import APP_BASE_URL,EMAIL,PASSWORD, CATEGORY_1, CATEGORY_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Driver\n",
    "driver = webdriver.Chrome(service = Service(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Running script\n",
      "Login Successful, Navigating To Inventory Page\n",
      "Done Extracting Table_headers ..... Commencing Data Scraping \n",
      " Done With Page 1 Extraction\n",
      "now fetching data from next page : 2\n",
      "now fetching data from next page : 3\n",
      "now fetching data from next page : 4\n",
      "now fetching data from next page : 5\n",
      "now fetching data from next page : 6\n",
      "now fetching data from next page : 7\n",
      "Seems there are no other pages: or  NoSuchElementException \n",
      "\n",
      " Moving to next Category: SD Finished Product.............\n",
      "now fetching data from next page : 2\n",
      "now fetching data from next page : 3\n",
      "now fetching data from next page : 4\n",
      "now fetching data from next page : 5\n",
      "now fetching data from next page : 6\n",
      "Seems there are no other pages: or  NoSuchElementException \n",
      "\n",
      " Moving to next Category: SD Finished Product.............\n",
      "Exporting 231 rows of data to Csv\n",
      "\n",
      "This took 129.1827654838562 seconds.\n",
      "Done with Webscraping\n"
     ]
    }
   ],
   "source": [
    "def login_and_get_table(EMAIL, PASSWORD, driver):\n",
    "    # driver.close()\n",
    "    driver.get(APP_BASE_URL+'/users/sign_in')\n",
    "    # find username/email field and send the username itself to the input field\n",
    "    driver.find_element(By.ID,\"user_email\").send_keys(EMAIL)\n",
    "    # find password input field and insert password as well\n",
    "    driver.find_element(By.ID,\"user_password\").send_keys(PASSWORD)\n",
    "    # click submit button which has tag \"mt-5\"\n",
    "    driver.find_element(By.CLASS_NAME,\"mt-5\").click()\n",
    "    sleep(2)\n",
    "    print('Login Successful, Navigating To Inventory Page')\n",
    "    # navigate to actual page to scrape data\n",
    "    driver.get(APP_BASE_URL+'/setup_inventory')\n",
    "    sleep(3)\n",
    "    driver.find_element(By.CLASS_NAME,\"ingredientsList\").click()\n",
    "    inventory_page = driver.page_source\n",
    "    raw_page_html = BeautifulSoup(inventory_page   ,'lxml')\n",
    "\n",
    "    table_headers = []\n",
    "    for table_header in raw_page_html.find_all('th'):\n",
    "        col_name = table_header.text\n",
    "        table_headers.append(col_name)   #Append all headers found to a list. \n",
    "    table_headers= table_headers[:-1]    #excluding last column header which had button links\n",
    "    return table_headers\n",
    "\n",
    "\n",
    "def extract_and_transform(table_headers):\n",
    "\n",
    "    Categs_to_Extract = [CATEGORY_1, CATEGORY_2]\n",
    "\n",
    "    extracted_inventory = pd.DataFrame()\n",
    "    for category in Categs_to_Extract:\n",
    "\n",
    "        driver.find_element(By.CLASS_NAME,\"filterByCategDropdown\").click()\n",
    "        sleep(2)\n",
    "        driver.find_element(By.PARTIAL_LINK_TEXT, category).click()\n",
    "\n",
    "        all_inventory_data = pd.DataFrame()\n",
    "        next_page_exists = True\n",
    "        page = 1\n",
    "        while (next_page_exists):\n",
    "            try:\n",
    "                page_info = driver.page_source\n",
    "                page_info_html =  BeautifulSoup(page_info,'lxml')\n",
    "                table_data = []\n",
    "                \n",
    "                for table_row in page_info_html.find_all('tr'):\n",
    "                    rows = {}\n",
    "                    for cell_data, table_header in zip(table_row.find_all('td'),table_headers):\n",
    "                        rows[table_header] = cell_data.text.replace('\\n','').strip()\n",
    "                    table_data.append(rows)\n",
    "                page_data_extracted = pd.DataFrame(table_data)\n",
    "                all_inventory_data = pd.concat([all_inventory_data,page_data_extracted])\n",
    "                sleep(5)\n",
    "                next_page = driver.find_element(By.XPATH,'//li[@class=\"page-item next\"]/a').click()\n",
    "                sleep(2)\n",
    "                page+=1\n",
    "                print('now fetching data from next page : {}'.format(page))\n",
    "            except Exception as E:\n",
    "                print('Seems there are no other pages: or ', type(E).__name__,'\\n\\n', 'Moving to next Category: {}.............'.format(Categs_to_Extract[1]))\n",
    "                extracted_inventory = pd.concat([extracted_inventory,all_inventory_data])\n",
    "                extracted_inventory.dropna(axis = 0, subset =['Name'],inplace=True)\n",
    "                next_page_exists = False\n",
    "                sleep(3)\n",
    "                break\n",
    "    return extracted_inventory\n",
    "\n",
    "def show_previews_and_export(extracted_inventory):\n",
    "    # excluded prints - propietary\n",
    "    # print('Preview of Top 5 Rows:','\\n','*---*'*10,'\\n\\n',extracted_inventory.head(10) ) \n",
    "    sleep(3)\n",
    "    # print('Preview of Bottom 5 Rows:','\\n','*---*'*10,'\\n\\n',extracted_inventory.tail(10) )\n",
    "    sleep(3)\n",
    "    print('Exporting {} rows of data to Csv'.format(len(extracted_inventory)))\n",
    "    extracted_inventory.to_csv('extracted_inventory_data.csv',index=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "  start_time = time.time()\n",
    "  print('Now Running script')\n",
    "  table_headers = login_and_get_table(EMAIL=EMAIL,PASSWORD= PASSWORD, driver=driver)\n",
    "  print('Done Extracting Table_headers ..... Commencing Data Scraping',\n",
    "            '\\n','Done With Page 1 Extraction')\n",
    "  extracted_inventory = extract_and_transform(table_headers)\n",
    "  show_previews_and_export(extracted_inventory)\n",
    "  driver.get(APP_BASE_URL+'/users/sign_out') #log out of app\n",
    "  print(\"\\nThis took %s seconds.\" % (time.time() - start_time))\n",
    "  print('Done with Webscraping')\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tmain()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2aafc8b317f9843068624b61f9604f1c5711d8aa460106b72f5abe10822f7ca8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
