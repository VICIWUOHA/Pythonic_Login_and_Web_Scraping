{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Running script\n",
      "login to App was successful with <Response [200]>\n",
      "now extracting data from page 1\n",
      "sleeping for some seconds before next request\n",
      "now extracting data from page 2\n",
      "sleeping for some seconds before next request\n",
      "now extracting data from page 3\n",
      "sleeping for some seconds before next request\n",
      "now extracting data from page 4\n",
      "successfully extracted 150 records from web app\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Copy of PYTHONIC APP LOGIN\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Using python to log into an app and get data\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import requests as r\n",
    "from bs4 import BeautifulSoup\n",
    "# import json\n",
    "import lxml\n",
    "from urllib.parse import urljoin\n",
    "from random import randint\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "from config import APP_BASE_URL,EMAIL,PASSWORD\n",
    "\n",
    "\n",
    "def login_to_app():\n",
    "  # Instantiate A login Session to be used for the operation\n",
    "  with r.Session() as s:\n",
    "\n",
    "    get_request = s.get(APP_BASE_URL+'/users/sign_in')\n",
    "\n",
    "    # each get request comes with a unique auth token, hence we use bs to get this token from the page content\n",
    "    login_page_content = BeautifulSoup(get_request.content,'html.parser')\n",
    "    token = login_page_content.find(\"input\", {\"name\":\"authenticity_token\"})[\"value\"]\n",
    "\n",
    "    # define payload within session using the token generated above\n",
    "    login_payload = {\n",
    "      'authenticity_token':token,\n",
    "      \"user[email]\":EMAIL,\n",
    "      \"user[password]\" :PASSWORD}\n",
    "\n",
    "    login_entry = s.post(APP_BASE_URL+'/users/sign_in', data= login_payload)\n",
    "    # print('login_entry.text')\n",
    "    if login_entry:\n",
    "      print('login to App was successful with {}'.format(login_entry))\n",
    "    else:\n",
    "      print('there was possibly an error')\n",
    "      \n",
    "  return s\n",
    "\n",
    "\n",
    "def get_page_content(s):\n",
    "\n",
    "  # Immediately after logging in, i tell the login what page i want to pull data from\n",
    "  # In this case, the \"reports/total_orders\" page-\n",
    "  # save this as a constant to call it easily\n",
    "  ORDERS_PAGE_URL = APP_BASE_URL+'/reports/total_orders'\n",
    "\n",
    "  orders_home_page= s.get(ORDERS_PAGE_URL)\n",
    "  # Make a get request to get the url of the app in the default order, page 2\n",
    "  orders_page_data = s.get(ORDERS_PAGE_URL+'?page=2&order_by=created_at&sort_by=ASC')\n",
    "\n",
    "  \"\"\"\n",
    "  Before Using BeautifulSoup, there was need to inspect the web page to know what classes/html objects\n",
    "  contained the necessary table to be scraped.\n",
    "  \"\"\"\n",
    "  # use beautifulsoup to get the content of the orders page \n",
    "  orders_page_content =BeautifulSoup(orders_page_data.content,'lxml')\n",
    "  # Then find the table tag with the class name \"table table-striped\"\n",
    "  orders_table = orders_page_content.find(\"table\", {\"class\":\"table table-striped\"})\n",
    "  # print(orders_table)  #TO PREVIEW raw html data\n",
    "  return orders_home_page,orders_page_content,orders_page_data\n",
    "\n",
    "\n",
    "def extract_data_to_dataframe(s,orders_home_page,orders_page_content,orders_page_data):\n",
    "  # BEGIN CREATION OF DATAFRAME\n",
    "  # Write all headers to a list\n",
    "  orders_home_page =orders_home_page\n",
    "  orders_page_data = orders_page_data\n",
    "\n",
    "  orders_table_headers = []\n",
    "  for table_header in orders_page_content.find_all('th'):\n",
    "    col_name =table_header.text\n",
    "    orders_table_headers.append(col_name)\n",
    "    \n",
    "  # Preview the table headers to be sure all were written to the list\n",
    "  # print(orders_table_headers)\n",
    "\n",
    "  # redefining get request to dynamically add the data on more pages if they exist\n",
    "  base_url_for_data = APP_BASE_URL+ f\"/reports/total_orders?\"\n",
    "  order = '&order_by=created_at&sort_by=ASC'\n",
    "  page = 0\n",
    "  # Instantiate new dataframe to generate\n",
    "  all_data = pd.DataFrame()\n",
    "  while True:\n",
    "      orders_dynamic_pages = s.get(base_url_for_data + f\"page={page}\" + order)\n",
    "      orders_page_paginated_content = BeautifulSoup(orders_dynamic_pages.content,'lxml')\n",
    "\n",
    "      table_data = []\n",
    "      for table_row in orders_page_paginated_content.find_all('tr'):\n",
    "        row = {}\n",
    "        for cell_data,table_header in zip(table_row.find_all('td'),orders_table_headers):\n",
    "          row[table_header] = cell_data.text.replace('\\n','').strip()\n",
    "        table_data.append(row)\n",
    "      page_data = pd.DataFrame(table_data)\n",
    "      page+=1 \n",
    "      print('now extracting data from page {}'.format(page))\n",
    "      if(page >=20)|len(page_data) < 1:\n",
    "          break\n",
    "      else:\n",
    "        all_data = pd.concat([all_data,page_data])\n",
    "        print('sleeping for some seconds before next request')\n",
    "        sleep(randint(15,20))\n",
    "  print(\"successfully extracted {} records from web app\".format(len(all_data)))\n",
    "  return all_data\n",
    "\n",
    "def write_to_file(all_data):\n",
    "  extracted_file = all_data.to_csv(\"extracted_records_{}.csv\".format(datetime.now().strftime('%Y-%m-%d_%H_%M_%S')))\n",
    "  return extracted_file\n",
    "\n",
    "\n",
    "def main():\n",
    "  print('Now Running script')\n",
    "  s = login_to_app()\n",
    "  orders_home_page,orders_page_content,orders_page_data = get_page_content(s)\n",
    "  all_data = extract_data_to_dataframe(s,orders_home_page,orders_page_content,orders_page_data)\n",
    "  write_to_file(all_data)\n",
    "  # return output           \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tmain()\n",
    "\n",
    "\n",
    "# print(len(all_data))\n",
    "# len(all_data)\n",
    "# all_data\n",
    "# # page\n",
    "# all_data.head()\n",
    "# # print(df)\n",
    "\n",
    "# df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "050ebf00ca2095756ddbca9eed01d506d4c764aa517f360c6e334e524f2a42be"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
